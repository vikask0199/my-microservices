# Create 2-nodes cluster one is the master node and one is the worker node
# -p=cluster name (Profile)
# local-cluster is our cutome cluster name
minikube start --nodes 2 -p local-cluster --driver=docker

# status of cluster
minikube status -p local-cluster
# In prodcuction we will not use minikube we will use kubeadm or eks

# kubectl syntax
kubectl[Command][Type][Name][Flags]
Command-> Create, get Desc, etc
Type-> pods, services, etc
Name-> resource
# exp-> kubectl get pods my-pod -o yaml
# -o yaml is to get the yaml file of the pod

# get all nodes
kubectl get nodes

# Get container in nodes
docker ps

# list down all the cluster
kubectl config get-contexts

# switch the cluster
kubectl config use-context minikube
kubectl config set-context local-cluster

# add node into cluster
minikube node add --worker -p local-cluster

# delete the node to the cluster
local-cluster-125dasfd->node name
minikube node delete local-cluster-125dasfd -p local-cluster

# minikube dashboda
minikube dashboard
minikube dashboard --url -p local-cluster

# Yaml-> YAML Ain't Markup Language. It is a serialization language
# serialization Language-> Whnever two system interacting with each other then need a common file format that is acceptable or readable by both. Yaml is a human readable file format exp yaml json xml 

# What is pods
# Pod is a group of containers that are running on a node

# create a pods for nginx image
kubectl run nginx-pod --image=nginx

# List of pods
kubectl get pods

# get all the resource type that containes pods
kubectl api-resources | grep pods

# kind
# kind is a type of object
# kind-> pod, service, deployment, etc

# Pass the config to the cluster using kubectl
kubectl apply -f location of config file

# delete the pods
kubectl delete pods nginx-pod

# Get the pods in the lable
# filter the pods
# -o indicate the output format (wide, yaml)
kubectl get pods -l team=xyz
kubectl get pods -l team=xyz -o wide

# describe the pods
kubectl describe pods nginx-pod

# Enter in the pod
kubectl exec -it nginx-pod -- bash
# exit for come-out

# Enter a specific container
kubectl exec -it nginx-pod -- bash -c -- bash

# Access cluster out side the port
kubectl port-forward nginx-pod 8080:80 (container port: localport)

# Logs of pods
kubectl logs nginx-pod

# Delete all the resources that is present into the yaml file
kubectl delete -f nginx-pod.yaml

# PSP respect to pod security policy

# high avalibility of our application
# we can use replica set
# Make sure that two pods always runnign with the labels (matchlabels and metadata labels should be same)
# get all replicas
# create two worker nodes and if one worker nodes goes down then the pods will execute into third node ((auto healing of the pods))
kubectl get rs


# delete node 
minikube node delete -p (nodename) -p local-cluster


# rollback and rollout
# we can hahdle the rollout and rollback from  into Deployment object so we dont need to create replicaset manually
# deployment craete replicaset->replicaset creates pods(Thats why pods is the smallest unit)
# We can create relicaset manually (through the worker node -> create worker node and if one worker node goes down then pods deploy into another worker nodes)


# resource type-> pods,service,deplyment
# delete everything from cluster (like resources and replicaset)
# all -> target the resources
# --all -> target the objects and other operations
kubectl delete all --all

# check that what is remaining and what is deleted
kubectl get all 


# filter the pods as per lable
kubectl get pods --show-labels

# scale the replicas (resourceType/ resourceName)(Always avoid to do manually)
kubectl scale --replicas=5 deployment/nginx-deployment

# Whenever we change anything into deployment.yaml and trigger new rollout and new replicaset created the old replica does't deleted and stored by kubernetes and we can rollback old version

# change the version of our applivation manually
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.21

# list down the history of rollout
kubectl rollout history deployment/nginx-deleted

# change cause record the changes after new version change(record the annotation)
kubectl set image deployment/nginx-deployment nginx-container=nginx:1.20 --record

# always try to annotate into deploymemt file
annotation:
    kubernetes.io/change-cause: "Updating the version of our applivation"

# Roll back to the previous version or application
# Revision -> whenever you check the rollout history then you will get the revision hitory (Change version)
kubectl rollout undo deployment/nginx-deployment --to-revision=1
# after that you can check the pods for the rollout affected or not 
kubectl describe pod nginx-deploymemt-nvjfxbhk | grep image

# Whenever a pod is assigned into the node the kube proxy agent allocate one ip to the pod or replica and these you can not access outside the cluster to resolve this issue service comes into picture
# If pods ip will change then service always target to appropriate ip of the pods
# service Provide load balancing 
# service is the entry point of the application
# Service follows zero down time deployments

# Cluset ip service -> Ip cannot be accessed out the cluster if you want to access the service u can call that service into a pod just using a command
kubectl exec -it podName --sh
curl clusterIP:port
# You can access through the name of the sevice
curl service-name:port

# port forwording of cluster
kubectl port-forword service/nginx-service 8083:8082

# Get the log of pod name
kubectl logs podName

# Get the pods associate with service
kubectl get endpoints

# Describe service
kubectl describe service/nginx-service

# U can use type node port so you can access out side the cluster on  your system 
# range of node port is (30000, 32767)

# We need an ingress controller to process the ingress rule
# Nginx ingrss controller and treafik and istio and HAProxy
# Enable ingress controller using cluster name
minikube addons enable ingress -p cluster-name

# get pods in namespace default namespace is ingress-nginx
kubectl get pods -n cluster-name

# in this case nginx create a node port service you can see the command using(add namespace at the end) if you are cluod it will create load balancer service intead of creating node port and this load balancer act as a entry point.
kubectl get svc -n ingress-nginx

# you can get the service info of ingress using
kubectl api-resources | grep ingress


# upadate the hosts database in system
sudo nano ./etc/hosts
# add here IP minikube ip and baseurl like 

192.168.49.2 nginx-demo.com
# after that close the file 

# add annotation here (any path define after the url that is use just for only identifire like http://todo.com/api/api/todos convert into http://todo.com/api/todos)
# path: /api/(.*)
annotations:
    nginx-ingress-kubernetes.io/rewrite-target: /$1

# To get all the urls
kubectl get ingress -n ingress-nginx

# In host based routing we are configure multiple host and whitelist the host ip service into hosts db after that you can access the endpoint through the host and no need to annotations
# If non of path patches then request goes to default http backend hosted on port 80


# generate self sign certificate 
openssl req -x509(typeofcertificate) -newkey rsa:4096(type) -sh256(algorithem) -nodes(no need password) -keyout tls.key -out tls.crt -subj "/CN=nginx-demo.com"(target domain) -days 365 (expiration date)
# now share this certificate to the ingress rule (all the dots replace by hyphon)
kubectl create tls  nginx-demo-com-tls --cert tsl.crt --key tls.key
# add into spec
tsl: 
    -secretName: nginx-demo-com-tls
    hosts:
        - "nginx-demo.com"
# after that apply the configuration and if you are unable to access the page after clicking on advanced then write thisisunsafe

# namespace basically use to group the application
# deafult namespaces
# default-> Resources when dont need to specify namespace explicitly
# kube-system-> This is the name space is for object created by controle plane.
# kube-public-> These resource are use to public resources. and not recomondede to use by user. this namse space is open to all users with read onlly access. 
# kube-node-lease-> Contains lease resource to send heartbeats of node. every node having a lease object. These lease object send heartbeats to the controle plane to help our cluster to determine the  avalibility of each node and take action when failures are detected. and each node associated with leas object

# create namespace
kubectl create namespace nginx-demo

# get all namespace
kubectl get all
kubectl get all --all-namespaces

# get all resources for namespace
kubectl  get all -n nginx-demo
# if your are not giving the namespace then it will return records from default namespaces

# chnage current active namespace it set by default namespace
kubectl config set-context --current --namespace=nginx-demo

# if you are into another namespaces and you want to access the resource of different namespace you can try 
kubectl exec -it pod/nginx-yhfghsg -- sh
curl todo-api-service:8080/api/todos (in this case you are unable to access the service that is running into different pod)
curl todo-api-service.(write the name of namespace you want to access like -> .todo.svc.cluster.local or you can go with .todo only):8080/api/todos

# delete namespace
kubectl delete ns namespaceName

# mongo db setup (create service and deployment)
kubectl port-forword svc/mongo-svc 32000:27017
# Enter inside the mongo pod
kubectl exec -it pod/mongo-0 -- /bin/bash
# check the process which mongo are running mongo db running into something PID
ps aux | grep mongo
ps aux
kill PID(mongodb PID) 
# if you will check the pod again  you will get that restart the conatiner not pod (So in this case whenevere container or craeted and deleted then data also deleted so we will store the data at thr pod level)
# To resolve this kubernets give a volume with the name of emptyDir
# emptyDir-> This is the volume which is created when pod is created and deleted when pod is deleted
# We will craete a volume in mongo deployment and mount the volume  with mongo container in mongo deployment
# we can mount the same volume with different containers
# if you are changing anything into the deployment file you need to do port-forwording again
# all the data store into (localtion where the pod is ruinnig)
# check
minikube ssh
sudo ls /var/lib/kubelet/pods
# you will get the list of pod id
kubectl get pod -o yaml
# you will get the pod uid and use into below command
kubectl get pod podname -o jsonpath='{.metadata.uid}'
# you will get the perticuler pod id using uppar command and uuid
sudo ls /var/lib/kubelet/add here pod id
# container host plugin volume
# list down the volume
sudo ls /var/lib/kubelet/podid/volumes
# you will get the list of volume and emptyDir
sudo ls /var/lib/kubelet/podid/volumes/kubernates.io~empty-dir
# This is not a perfect solution to store data into the emptyDir because if the pod is deleted then data will be deleted also to resolve this is hostPath comes into the picture
# upadte the mognodb deployement and update the emptyDir with host path and upadte everything into as per kubernets documentation
# in this case if the pods rinning into different node so then can not share the same mongodb to the with all pods if the node is deleted then data is deleted
# to resolve this issue we will use Persistent volume. Kubernets offer three persist data to prevent the dataloss (Persistent Volume, Persistent volume claim, Storage class) (AWS EBS, NFS server )
# setup persistent volume
# create Deployemnt file (You will get the basic info about persistence volume)
kubectl api-resource | grep Persistent
# Persistnet valume -> We can create multiple persistent valume into the cluster but how do we use this persistent volume into mongo pod that we have created on cloud so to resolve this persistent volume comes into the picture
# Persistnet valume claim -> this is another resources of kubernets this is maintain how much storage our pod needs we can not use persistent volume into the directly pod
kubectl get PersistnetVolumes
# create mongo pvc
kubectl create -f mongo-pvc.yaml
kubectl get pvc
# if the directoy not found into the minikube node
minikube ssh
mkdir /storage/data -p
# delete the pvc
kubectl delete pvc mongo-pvc
# We can not deleet direclty first we need to delete the pod
kubectl delete -f deployment.yaml
# You will see the pv
kubectl get pv
# if pv status is released then the connection  between pvc and pv ditrurrupted. If you want to delete the pv just 
kubectl delete pv pvName
# first deploy pv.yaml then pvc.yaml then deploy the respceted service and then expose the pv port
# If you want to access the mongo then shortcut command
ctrl+r 
# The pod is created ny the dEVELOPER AND pv craeted by the administator. In this case pods created dynamicallly whenever need pv to resolve this storage class comes into the picture
# Storage class define that how the pvs created dynamically 
# Basic info of storage 
kubectl api-resources | grep Storage
# Check the persistent volume claim and persistent volume
kubectl api-resources | grep Persistent
# Get the storage 
kubectl get sc
# standard storage class always created whenever cluster is craeted its default
# whenever you craete the storage class after that you can create the pvc volume and after craeting pvc volume the new pc craeted automatically
kubectl get pv


# statful and stateless
# Each pods have own persistent volume 
# cluster IP is none is called hedless service
# After apply none each pod has its dns service
mongo-0.mongo.deafult.svc.cluster.local:27017
# mongo-o => PondName
# mongo => service name
# deafult => namespace
# svc.cluster.local => cluster name
# 27017 => port

# get basic info statfulset
kubectl api-resources | grep statefulset
# apply statefulset
kubectl apply -f statefulset.yaml
# check what is going on after applying statefulset
kubectl get pods -w
# scale maunally
kubectl scale sts mongo --replicas=10
kubectl scale sts mongo --replicas=3
# pods delete automatically 
mongo delete po mongo-0
# get all persistence volume claim
kubectl get pvc
# Total number of persistente volume created is the max number of replicas statefulset
# Describe the pod
kubectl describe po mongo-0 | grep volume
# whenever service is stopped and restart then the same persistent apply to the same pod
# enter the mongo-0 pod
kubectl exec -it mongo-0 -- mongo

# we can not access direclty mongo db out side the cluster
# we have to follow rs initiate command into pod
# rs.initiate(
#     {
#         _id: "rs0",
#         members: [
#             { _id: 0, host: "mongo-0.mongo.default.svc.cluster.local:27017},
#             { _id: 1, host: "mongo-1.mongo.default.svc.cluster.local:27017}
#         ]
#     }
# )
rs.status()
# use test db add record into db and go to secondry node
kubectl exec -it mongo-1 -- mongo
# first enable the reads
rs.slaveOk()
# after that get the record from the database

# kubectl provides two special type volume
# Provide the dynamic cofiguration through the config map
# get basic info of configmap
kubectl api-resources | grep configmap
# Apply configmap
# get configmap
kubectl get cm
# and use the config map into deployament or service file
# check the config amp mounted correectly mounted or not
kubectl exec -it mongo-0 -- bash
# see configuration file is mounted or not
cat /etc/mongo/mongodb.conf
# configmap can not update env variable automatically so restart the pod and you will get the environment varibale automatically
# if not updated then delete the pod and pod is automatically create and is this case u will get the config map automatically update the configmap
# provide the encrypted password
echo -n password | base64
# After generated the password apply into mongo-secret.yaml and update the statefulset.yaml
kubectl exce -it mongo-0 -- /bin/sh
# You can get the env data directly type env inside the bash
env
# now you can chnage the password directly
echo -n password123 | base64
# docode the password 
echo -n bdbjhfkjhksdj | base64 --decode
# Its your choice to choose configmap or secret

# Probes-> Basically its define the behaviour and autohealing of the resouces
# Through the probes kubernets investigate the pods working correctly or not
# Liveness probe-> It check the pod is working correctly or not(it returns 1=Failure, 0=Healthy) you have to setup the command into statefulset.yaml or deployment.yaml
# Readiness probe-> It check the pod is ready to accept the request or not(It returns the success and failed)(if pod return the failed then rediness prob remove the connection to that pod until the pod is not resturn to success at these time pod is not getting any traffic) as rediness failed the pod is removed from the service
# Startup probe-> It check the pod is ready to accept the request or not(its define the delay of the execution of Liveness probe and rediness probe not sending any probes until the container is  not returned that container is ready to accept the probes and request) if statrtup probes is success then rediness and liveness probe is execute
# You can update restart policy as per your requirment

# cpu/memory
# 1cpu = 1vCPU in Aws
# 1cpu = 1Core in GCP
# 1cpu = 1VCore in Azure
# 1cpu = 1Hyperthread on bare-metal
# cpu is compressible
# memory is not compressable
# If the process take the maximum memory process will be killed
# You can not update the hardware that you have allocated during memery allocation and CPU allocation
#  you can seee the matric but first you have to enable matric server
minikube addons enable metrics-server
# you can see the matric server is running AND OPEN THE TOP  POD MEMORY USAGES 
kubectl top pods
# with the help of Resource quota we can define the namesapace limits and no of deployments and memory usage and property of 

# kubernets scheduler are responsible to craete the pod 




